name: Scrape Islam Articles Manual

on:
  # Allow manual trigger (must be first for visibility)
  workflow_dispatch:
    inputs:
      scrape_mode:
        description: 'Scraping mode'
        required: true
        default: 'incremental'
        type: choice
        options:
        - incremental
        - historical
        - full_historical
      max_pages:
        description: 'Maximum pages to scrape (optional)'
        required: false
        type: number
      delay:
        description: 'Delay between requests (seconds)'
        required: false
        default: '2'
        type: number
  
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
    # Run weekly full scrape on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

# Grant necessary permissions for the workflow to push changes and create releases
permissions:
  contents: write    # Allow pushing commits and creating releases
  issues: write      # Allow creating issues on failure
  actions: read      # Allow reading workflow artifacts

env:
  PYTHONUNBUFFERED: 1

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history to preserve database
        fetch-depth: 0
        # Use GITHUB_TOKEN with write permissions granted above
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install GitHub CLI
      run: |
        # Install GitHub CLI for release creation
        curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
        sudo apt update
        sudo apt install gh -y

    - name: Determine scrape mode
      id: mode
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo "mode=${{ github.event.inputs.scrape_mode }}" >> $GITHUB_OUTPUT
          echo "max_pages=${{ github.event.inputs.max_pages }}" >> $GITHUB_OUTPUT
          echo "delay=${{ github.event.inputs.delay }}" >> $GITHUB_OUTPUT
        elif [[ "${{ github.event.schedule }}" == "0 2 * * 0" ]]; then
          echo "mode=weekly_full" >> $GITHUB_OUTPUT
          echo "max_pages=" >> $GITHUB_OUTPUT
          echo "delay=3" >> $GITHUB_OUTPUT
        elif [[ "${{ github.event.schedule }}" == "0 6 * * *" ]]; then
          echo "mode=daily" >> $GITHUB_OUTPUT
          echo "max_pages=10" >> $GITHUB_OUTPUT
          echo "delay=2" >> $GITHUB_OUTPUT
        else
          echo "mode=incremental" >> $GITHUB_OUTPUT
          echo "max_pages=5" >> $GITHUB_OUTPUT
          echo "delay=2" >> $GITHUB_OUTPUT
        fi

    - name: Show scraping plan
      run: |
        echo "ðŸ” Scraping Mode: ${{ steps.mode.outputs.mode }}"
        echo "ðŸ“„ Max Pages: ${{ steps.mode.outputs.max_pages }}"
        echo "â±ï¸  Delay: ${{ steps.mode.outputs.delay }}s"
        echo "ðŸ• Triggered by: ${{ github.event_name }}"
        if [[ "${{ github.event_name }}" == "schedule" ]]; then
          echo "ðŸ“… Schedule: ${{ github.event.schedule }}"
        fi

    - name: Run Islam Articles Scraper
      run: |
        # Set parameters based on mode
        MODE="${{ steps.mode.outputs.mode }}"
        MAX_PAGES="${{ steps.mode.outputs.max_pages }}"
        DELAY="${{ steps.mode.outputs.delay }}"
        
        # Build command
        CMD="python github_actions_islam_scraper.py"
        
        case $MODE in
          "incremental"|"daily")
            CMD="$CMD --mode incremental"
            if [[ -n "$MAX_PAGES" ]]; then
              CMD="$CMD --max-pages $MAX_PAGES"
            fi
            ;;
          "historical"|"full_historical")
            CMD="$CMD --mode historical"
            if [[ -n "$MAX_PAGES" ]]; then
              CMD="$CMD --max-pages $MAX_PAGES"
            fi
            ;;
          "weekly_full")
            CMD="$CMD --mode historical --max-pages 50"
            ;;
        esac
        
        if [[ -n "$DELAY" ]]; then
          CMD="$CMD --delay $DELAY"
        fi
        
        echo "ðŸš€ Running: $CMD"
        $CMD

    - name: Generate summary report
      run: |
        python -c "
        import json
        import sqlite3
        import os
        from datetime import datetime

        # Check if database exists
        if not os.path.exists('islam_articles.db'):
            print('âš ï¸  No database found yet. This might be the first run.')
            summary = f'''## ðŸ“Š Scraping Summary - {datetime.now().strftime(\"%Y-%m-%d %H:%M UTC\")}

            **ðŸ†• First Run:**
            - Database created
            - Initial scraping completed
            - Check artifacts for results
            '''
            with open('scraping_summary.md', 'w') as f:
                f.write(summary)
            print(summary)
        else:
            # Read database stats
            conn = sqlite3.connect('islam_articles.db')
            cursor = conn.cursor()

            cursor.execute('SELECT COUNT(*) FROM articles')
            total_articles = cursor.fetchone()[0]

            cursor.execute('SELECT COUNT(*) FROM scraping_log WHERE run_date > datetime(\"now\", \"-1 day\")')
            recent_runs = cursor.fetchone()[0]

            cursor.execute('SELECT SUM(new_articles) FROM scraping_log WHERE run_date > datetime(\"now\", \"-1 day\")')
            new_today = cursor.fetchone()[0] or 0

            cursor.execute('SELECT matching_keyword, COUNT(*) FROM articles GROUP BY matching_keyword ORDER BY COUNT(*) DESC LIMIT 5')
            top_keywords = cursor.fetchall()

            conn.close()

            # Generate summary
            summary = f'''## ðŸ“Š Scraping Summary - {datetime.now().strftime(\"%Y-%m-%d %H:%M UTC\")}

            **ðŸ“ˆ Database Statistics:**
            - Total Articles: {total_articles}
            - New Articles Today: {new_today}
            - Scraping Runs Today: {recent_runs}

            **ðŸ” Top Keywords:**
            ''' + '\\n'.join([f'- {keyword}: {count} articles' for keyword, count in top_keywords])

            # Write to file
            with open('scraping_summary.md', 'w') as f:
                f.write(summary)

            print(summary)
        "

    - name: Generate all file formats
      run: |
        # Check if database exists before generating files
        if [ ! -f "islam_articles.db" ]; then
          echo "âš ï¸  Database not found. Skipping file generation on first run."
          mkdir -p articles_archive
          echo "First run - no articles to generate yet" > articles_archive/README.txt
        else
          # Generate HTML, Markdown, and Website files from database
          python scripts/generate_files_from_database.py --database islam_articles.db --output articles_archive

          # Create compressed archive for easy download
          tar -czf complete-articles-archive.tar.gz articles_archive/

          # Show what was created
          echo "ðŸ“ Generated files:"
          ls -la articles_archive/
          if [ -d "articles_archive/html_articles" ]; then
            echo "ðŸ“„ HTML files: $(find articles_archive/html_articles -name '*.html' | wc -l)"
          fi
          if [ -d "articles_archive/markdown_articles" ]; then
            echo "ðŸ“ Markdown files: $(find articles_archive/markdown_articles -name '*.md' | wc -l)"
          fi
        fi

    - name: Upload complete articles archive
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: complete-articles-archive-${{ github.run_number }}
        path: |
          articles_archive/
          complete-articles-archive.tar.gz
          islam_articles.db
          all_islam_articles.json
          scraping_summary.md
          *.log
        retention-days: 90
        if-no-files-found: warn

    - name: Upload website for GitHub Pages
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: github-pages-website
        path: articles_archive/website/
        retention-days: 30
        if-no-files-found: warn

    - name: Commit and push changes
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add files
        git add -A
        
        # Check if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          # Commit with timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
          git commit -m "ðŸ“Š Update Islam articles database - $TIMESTAMP
          
          Mode: ${{ steps.mode.outputs.mode }}
          Run: #${{ github.run_number }}
          
          ðŸ¤– Automated scraping via GitHub Actions"
          
          # Push changes
          git push
          echo "âœ… Changes committed and pushed"
        fi

    - name: Create release on major updates
      if: steps.mode.outputs.mode == 'weekly_full' || steps.mode.outputs.mode == 'historical'
      run: |
        # Check if database and files exist before creating release
        if [ ! -f "islam_articles.db" ]; then
          echo "âš ï¸  Database not found. Skipping release creation."
          exit 0
        fi

        if [ ! -f "complete-articles-archive.tar.gz" ]; then
          echo "âš ï¸  Archive not found. Skipping release creation."
          exit 0
        fi

        # Get article count and file counts
        ARTICLE_COUNT=$(python -c "
        import sqlite3
        conn = sqlite3.connect('islam_articles.db')
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM articles')
        print(cursor.fetchone()[0])
        conn.close()
        ")

        # Only create release if we have articles
        if [ "$ARTICLE_COUNT" -eq 0 ]; then
          echo "âš ï¸  No articles in database. Skipping release creation."
          exit 0
        fi

        HTML_COUNT=$(find articles_archive/html_articles -name '*.html' 2>/dev/null | wc -l)
        MD_COUNT=$(find articles_archive/markdown_articles -name '*.md' 2>/dev/null | wc -l)
        ARCHIVE_SIZE=$(du -sh complete-articles-archive.tar.gz 2>/dev/null | cut -f1 || echo "N/A")
        
        # Create release tag and notes
        TAG="archive-$(date +%Y%m%d-%H%M)"

        # Get timeline coverage
        TIMELINE=$(python -c "
        import sqlite3
        conn = sqlite3.connect('islam_articles.db')
        cursor = conn.cursor()
        cursor.execute('SELECT MIN(publish_date_parsed), MAX(publish_date_parsed) FROM articles WHERE publish_date_parsed IS NOT NULL')
        min_date, max_date = cursor.fetchone()
        if min_date and max_date:
            print(f'{min_date[:4]} - {max_date[:4]}')
        else:
            print('Various years')
        conn.close()
        ")

        # Build list of files to attach
        FILES_TO_ATTACH=""
        [ -f "complete-articles-archive.tar.gz" ] && FILES_TO_ATTACH="$FILES_TO_ATTACH complete-articles-archive.tar.gz"
        [ -f "islam_articles.db" ] && FILES_TO_ATTACH="$FILES_TO_ATTACH islam_articles.db"
        [ -f "all_islam_articles.json" ] && FILES_TO_ATTACH="$FILES_TO_ATTACH all_islam_articles.json"
        [ -f "scraping_summary.md" ] && FILES_TO_ATTACH="$FILES_TO_ATTACH scraping_summary.md"

        # Create release notes
        RELEASE_NOTES="**Complete article archive with all formats**

        ðŸ“Š **Statistics:**
        - Total Articles: $ARTICLE_COUNT Islam-related articles
        - HTML Files: $HTML_COUNT individual readable articles
        - Markdown Files: $MD_COUNT clean text versions
        - Archive Size: $ARCHIVE_SIZE compressed
        - Scrape Mode: ${{ steps.mode.outputs.mode }}
        - Generated: $(date -u +'%Y-%m-%d %H:%M UTC')

        ðŸ“ **Download Options:**
        - \`complete-articles-archive.tar.gz\` - **Everything** (HTML, Markdown, Website, Database)
        - \`islam_articles.db\` - SQLite database only
        - \`all_islam_articles.json\` - JSON export only

        ðŸš€ **Usage:**
        1. Download \`complete-articles-archive.tar.gz\`
        2. Extract: \`tar -xzf complete-articles-archive.tar.gz\`
        3. Browse: Open \`articles_archive/website/index.html\`
        4. Read: Individual files in \`articles_archive/html_articles/\`

        ðŸ“ˆ **Timeline Coverage:** $TIMELINE

        ðŸ¤– Auto-generated by GitHub Actions"

        # Create release with available files (skip if gh not available)
        echo "ðŸ“¦ Attaching files: $FILES_TO_ATTACH"
        if command -v gh >/dev/null 2>&1; then
          gh release create "$TAG" \
            --title "ðŸ“š Complete Islam Articles Archive - $(date '+%Y-%m-%d')" \
            --notes "$RELEASE_NOTES" \
            $FILES_TO_ATTACH
        else
          echo "âš ï¸ GitHub CLI not available, skipping release creation"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Send notification on failure
      if: failure()
      run: |
        echo "âŒ Scraping failed!"
        echo "::error::Islam articles scraping failed. Check the logs for details."
        
        # Create issue on failure (optional, skip if gh not available)
        if [[ "${{ steps.mode.outputs.mode }}" == "historical" ]] && command -v gh >/dev/null 2>&1; then
          gh issue create \
            --title "ðŸš¨ Islam Articles Scraper Failed - $(date '+%Y-%m-%d')" \
            --body "The Islam articles scraper failed during a ${{ steps.mode.outputs.mode }} run.
            
            **Details:**
            - Run: #${{ github.run_number }}
            - Mode: ${{ steps.mode.outputs.mode }}
            - Time: $(date -u +'%Y-%m-%d %H:%M UTC')
            
            Please check the [action logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details." \
            --assignee "${{ github.actor }}" || true
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}